{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Import and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# All files from teslamotorsinc-my.sharepoint.com are placed under energy_data\n",
    "# in this project. These files are provided seprately.\n",
    "DATA_FILE_LOCATION = os.path.join(os.getcwd(), \"energy_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather data filenames for import from the file system\n",
    "signal_filenames = [f for f in os.listdir(DATA_FILE_LOCATION) if \"signals_data\" in f]\n",
    "alert_filenames = [f for f in os.listdir(DATA_FILE_LOCATION) if \"alerts_data\" in f]\n",
    "\n",
    "if len(signal_filenames)==0 or len(alert_filenames)==0:\n",
    "    raise Exception(\"ERROR: CSV data files were not found. These should be added under /energy_data\")\n",
    "\n",
    "# Import signals and alerts from all the files into dataframes.\n",
    "# There is not much data so this should be okay\n",
    "# Otherwise I would tackle this day by day, as pandas is all in-memory\n",
    "signal_df = helper.import_csv_files(DATA_FILE_LOCATION, signal_filenames)\n",
    "alert_df = helper.import_csv_files(DATA_FILE_LOCATION, alert_filenames)\n",
    "\n",
    "# Also clean the signal data for unrealistic temperatures as explained in input_validation.ipynb\n",
    "signal_df = helper.clean_temperature_data(signal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge #1\n",
    "Using the signals_data files, provide a list of components that have an average temeprature above 105 degrees during any 30 second period. Perform this analysis for all temperature signals in the dataset.\n",
    "\n",
    "It doesn't make much sense to me to choose arbitrary 30 second time periods, like we might do with hourly or daily aggregations. So I will instead do rolling calculations with a sliding window of 30 seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
